{
  "hash": "d6d032c00ed9ad00b86c1d42a3836432",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Untitled\"\nexecute: \n  echo: true\n  collapse: true\nknitr:\n  opts_chunk: \n    collapse: true\n    comment: \"#>\"\n---\n\n\n\n\n\n# Meta-analysis {.section}\n\n## Meta-analysis\n\n- The meta-analysis is a statistical procedure to combine evidence from a group of studies.\n\n. . .\n\n- The idea is to \"switch\" the statistical unit from e.g., participants to studies\n\n. . .\n\n- The motto could be that (appropriately) combining similar studies with a similar aim is the best way to understand something about the phenomenons\n\n## Meta-analysis and Systematic Review\n\nUsually a meta-analysis work follows these steps:\n\n1. **Identify the research question**: is the treatment *x* effective?, Does the experimental effect `y` exist?\n2. **Define inclusion/exclusion criteria**: From the research question (1), keep only e.g., randomized controlled trials, studies with healthy participants, etc.\n3. **Systematically search for studies**: Analyze the literature to find all relevant studies\n4. **Extract relevant information**: Read, extract and organize relevant information e.g., sample size, treatment type, age, etc.\n5. **Summarize the results**: Create a narrative (flowcharts, tables, etc.) summary of included studies. This is the Systematic review part.\n6. **Choose an effect size**: Choose a way to standardize the effect across included studies\n7. **Meta-analysis model**: Choose and implement a meta-analysis model\n8. **Interpret and report results**\n\n## Unstandardized effect sizes\n\nThe basic idea of an effect size is just using the raw measure. For example studies using reaction times we can calculate the difference between two conditions as $\\overline X_1 - \\overline X_2$:\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](intrometa_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n\n## Unstandardized effect sizes\n\nBut another study (with the same research question) could use another measure, e.g., accuracy. We can still (not the best strategy but) compute the difference between the group means.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](intrometa_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n\n\n## Unstandardized effect sizes\n\nClearly we cannot directly compare the two effects but we need to standardize the measure.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrt_plot + acc_plot\n```\n\n::: {.cell-output-display}\n![](intrometa_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n\n## Standardized effect sizes\n\nTo compare results from different studies, we should use a common metrics. Frequently meta-analysts use *standardized* effect sizes. For example the Pearson correlation or the Cohen's $d$.\n\n$$\nr = \\frac{\\sum{(x_i - \\bar{x})(y_i - \\bar{y})}}{\\sqrt{\\sum{(x_i - \\bar{x})^2}\\sum{(y_i - \\bar{y})^2}}}\n$$ {#eq-correlation}\n\n$$\nd = \\frac{\\bar{x_1} - \\bar{x_2}}{s_p}\n$$\n\n$$\ns_p = \\sqrt{\\frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 + n_2 - 2}}\n$$\n\n## Standardized effect sizes\n\nThe advantage of standardized effect size is that regardless the original variable, the interpretation and the scale is the same. For example the pearson correlation ranges between -1 and 1 and the Cohen's $d$ between $- \\infty$ and $\\infty$ and is interpreted as how many standard deviations the two groups/conditions differs.\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nS <- matrix(c(1, 0.7, 0.7, 1), nrow = 2)\nX <- MASS::mvrnorm(100, c(0, 2), S, empirical = TRUE)\n\npar(mfrow = c(1,2))\nplot(X, xlab = \"x\", ylab = \"y\", cex = 1.3, pch = 19,\n     cex.lab = 1.2, cex.axis = 1.2,\n     main = latex2exp::TeX(sprintf(\"$r = %.2f$\", cor(X[, 1], X[, 2]))))\nabline(lm(X[, 2] ~ X[, 1]), col = \"firebrick\", lwd = 2)\n\n\nplot(density(X[, 1]), xlim = c(-5, 7), ylim = c(0, 0.5), col = \"dodgerblue\", lwd = 2,\n     main = latex2exp::TeX(sprintf(\"$d = %.2f$\", lsr::cohensD(X[, 1], X[, 2]))),\n     xlab = \"\")\nlines(density(X[, 2]), col = \"firebrick\", lwd = 2)\n```\n\n::: {.cell-output-display}\n![](intrometa_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n\n## Standardized vs unstandardized\n\nThe main difference is (usually) the absence of a effect-size-variance relationship for unstandardized effects. For example, the variance of the difference between two groups is:\n\n$$\nV_d = \\frac{s^2_1}{n_1} + \\frac{s^2_2}{n_2}\n$$ {#eq-var-umd}\n\nWhile the variance of a Cohen's $d$ can be calculated as:\n\n$$\nV_d = \\frac{n_1 + n_2}{n_1 n_2} + \\frac{d^2}{2(n_1 + n_2)}\n$$\n## Standardized vs unstandardized\n\nIn this [amazing blog post](https://www.jepusto.com/alternative-formulas-for-the-smd/) James Pustejovsky explained where the equations comes from. Basically, the $\\frac{n_1 + n_2}{n_1 n_2}$ term is the same as the $\\frac{s^2_1}{n_1} + \\frac{s^2_2}{n_2}$ while the extra $\\frac{d^2}{2(n_1 + n_2)}$ is for the non-centrality induced by the standardized difference.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn <- c(10, 50, 100)\nd <- seq(0, 2, 0.001)\n\ndd <- expand.grid(n = n, d = d)\n\ndd$vumd <- with(dd, 1/n + 1/n)\ndd$vd <- with(dd, (n + n) / (n * n) + d^2/(2 * (n + n)))\n\ntidyr::pivot_longer(dd, 3:4) |> \n  ggplot(aes(x = d, y = value, color = name, linetype = factor(n))) +\n  geom_line() +\n  labs(linetype = \"Sample Size\",\n       color = NULL)\n```\n\n::: {.cell-output-display}\n![](intrometa_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n\n## Combining studies\n\nLet's imagine to have $k = 10$ studies and we want to compute the aggregated effect. We calculate the effect size as the Cohen's $d$.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nk <- 10\n# just generating k random values between -0.3 and 1\nd <- round(runif(k, -0.3, 1), 2)\ndat <- data.frame(id = 1:k, d)\ndat\n#>    id     d\n#> 1   1  0.65\n#> 2   2  0.37\n#> 3   3  0.17\n#> 4   4 -0.06\n#> 5   5  0.19\n#> 6   6  0.21\n#> 7   7  0.58\n#> 8   8  0.73\n#> 9   9  0.11\n#> 10 10 -0.01\n```\n:::\n\n\n\n## Combining studies\n\nWe can take the average effect size and considering it as a huge study. This can be considered the best way to combine the effects.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(dat$d)\n#> [1] 0.294\n```\n:::\n\n\n\n. . .\n\nWhat do you think? Can be considered an appropriate way to combine the effects? What is missing?\n\n## Weighting studies\n\nWhat if I add a sample size column? Now regardless the effect size $d$, some studies have small/large sample size. Would you trust more a $d = 0.5$ with 1000 participants or with 10 participants?\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat$n <- round(runif(k, 10, 100))\ndat\n#>    id     d  n\n#> 1   1  0.65 96\n#> 2   2  0.37 83\n#> 3   3  0.17 44\n#> 4   4 -0.06 69\n#> 5   5  0.19 99\n#> 6   6  0.21 97\n#> 7   7  0.58 19\n#> 8   8  0.73 77\n#> 9   9  0.11 39\n#> 10 10 -0.01 55\n```\n:::\n\n\n\n## Weighting studies\n\nBasically we need to find a value (called weight $w_i$) that assign more trust to a study because it provide more information. The simplest weights are just the sample size. Thus we can compute a weighted average instead of the standard average.\n\n$$\n\\overline D_w = \\frac{\\sum^k_{i = 1}{w_id_i}}{\\sum^k_{i = 1}{w_i}}\n$$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwi <- dat$n\nsum(dat$d * wi) / sum(wi)\n#> [1] 0.3047198\n# weighted.mean(dat$d, wi)\n```\n:::\n\n\n\n## Effect size sampling variability {#sec-effsize-se}\n\n::: {.panel-tabset}\n\nCrucially, we can calculate also the **sampling variability** of each effect size. The **sampling variability** is the precision of estimated value.\n\n\n### Formula\n\nFor example, there are multiple methods to estimate the Cohen's $d$ sampling variability. For example:\n\n$$\nV_d = \\frac{n_1 + n_2}{n_1 n_2} + \\frac{d^2}{2(n_1 + n_2)}\n$$\n\nEach effect size has a specific formula for the sampling variability. The sample size is usually the most important information. Studies with high sample size have low sampling variability.\n\n### Plot\n\nAs the sample size grows and tends to infinity, the sampling variability approach zero.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](intrometa_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n\n:::\n\n## Unstandardized effect sizes\n\nFor the examples and plots I'm going to use simulated data^[If you are interested in meta-analysis simulation we wrote a preprint [https://psyarxiv.com/br6vy/](https://psyarxiv.com/br6vy/)]. We simulate *unstandardized* effect sizes (UMD) because the computations are easier and the estimator is unbiased [e.g., @Viechtbauer2005-zt]\n\nMore specifically we simulate hypothetical studies where two independent groups are compared:\n\n$$\n\\Delta = \\overline{X_1} - \\overline{X_2}\n$$ {#eq-umd}\n\n$$\nSE_{\\Delta} = \\sqrt{\\frac{s^2_1}{n_1} + \\frac{s^2_2}{n_2}}\n$$\n\nWith $X_{1_i} \\sim \\mathcal{N}(0, 1)$ and $X_{2_i} \\sim \\mathcal{N}(\\Delta, 1)$\n\nThe main advantage is that, compared to standardized effect size, the sampling variability do not depends on the effect size itself, simplifying the computations.\n\n## Notation {.smaller}\n\nMeta-analysis notation is a little bit inconsistent in textbooks and papers. We define here some rules to simplify the work.\n\n- $k$ is the number of studies\n- $n_j$ is the sample size of the group $j$ within a study\n- $y_i$ are the observed effect size included in the meta-analysis\n- $\\sigma_i^2$ are the observed sampling variance of studies and $\\epsilon_i$ are the sampling errors\n- $\\theta$ is the equal-effects parameter (see @eq-ee1)\n- $\\delta_i$ is the random-effect (see @eq-re-mod2)\n- $\\mu_\\theta$ is the average effect of a random-effects model (see @eq-re-mod1)\n- $w_i$ are the meta-analysis weights (e.g., see @eq-wi)\n- $\\tau^2$ is the heterogeneity (see @eq-re-mod2)\n- $\\Delta$ is the (generic) population effect size\n- $s_j^2$ is the variance of the group $j$ within a study\n\n# Simulating a single study\n\n## Simulating a single study - UMD\n\nTo simulate a single study using a UMD we need to generate data according to the appropriate model. Here we have a difference between two groups. We can assume that the two groups comes from a normal distribution where group 1 $g_1 \\sim \\mathcal{N}(0, 1)$ and group 2 $g_2 \\sim \\mathcal{N}(D, 1)$ where $D$ is the effect size. Then using Equations [-@eq-var-umd; -@eq-umd] we can estimate the effect size and the variance.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nD <- 1  # effect size\nn <- 50 # sample size\ng1 <- rnorm(n, mean = 0, sd = 1)\ng2 <- rnorm(n, mean = D, sd = 1)\n\n# effect size\nmean(g2) - mean(g1)\n#> [1] 1.222825\n\n# variance\nvar(g1)/n + var(g2)/n\n#> [1] 0.04994345\n```\n:::\n\n\n\n## Simulating a single study - UMD\n\nFor simplicity we can wrap everything within a function:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# default sd = 1\nsim_umd <- function(n1, n2 = NULL, D, sd = 1){\n  if(is.null(n2)) n2 <- n1 # same to n1 if null \n  g1 <- rnorm(n1, mean = 0, sd = sd)\n  g2 <- rnorm(n2, mean = D, sd = sd)\n  yi <- mean(g2) - mean(g1)\n  vi <- var(g1)/n + var(g2)/n\n  data.frame(yi, vi)\n}\n\nsim_umd(100, D = 0.5)\n#>          yi         vi\n#> 1 0.4590987 0.04222352\nsim_umd(50, D = 0.1)\n#>           yi         vi\n#> 1 -0.1512217 0.04949939\n```\n:::\n\n\n\n## Simulating a single study - UMD\n\nWe can also generate a large number of studies and check the distribution of effect size and sampling variances. Note that the real $D = 1$ and the real variance $V_D = 1/50 + 1/50 = 0.04$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstudies <- replicate(1000, sim_umd(n1 = 50, D = 1), simplify = FALSE) # simplify = FALSE return a list\nstudies <- do.call(rbind, studies) # to dataframe\nhead(studies)\n#>         yi         vi\n#> 1 1.060760 0.03251778\n#> 2 1.044612 0.04657504\n#> 3 1.170203 0.04537363\n#> 4 1.236175 0.04209411\n#> 5 1.242883 0.04583768\n#> 6 1.035602 0.03305830\n```\n:::\n\n\n\n## Simulating a single study - UMD\n\nThen we can plot the sampling distributions:\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](intrometa_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\n\n## Simulating a single study - SMD\n\nThe idea is the same when simulating a SDM but we need extra steps. Let's adjust the previous function:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsim_smd <- function(n1, n2 = NULL, D){\n  if(is.null(n2)) n2 <- n1 # same to n1 if null \n  g1 <- rnorm(n1, mean = 0, sd = 1)\n  g2 <- rnorm(n2, mean = D, sd = 1)\n  \n  v1 <- var(g1)\n  v2 <- var(g2)\n  \n  # pooled standard deviation\n  sp <- sqrt((v1 * (n1 - 1) + v2 * (n2 - 1)) / (n1 + n2 - 2))\n  \n  yi <- (mean(g2) - mean(g1)) / sp\n  vi <- (n1 + n2) / (n1 * n2) + yi^2/(2*(n1 + n2))\n  data.frame(yi, vi)\n}\n```\n:::\n\n\n\n## Simulating a single study - SMD\n\nWhen working with SMD, calculating the sampling variance can be challenging. @Veroniki2016-nw identified 16 different estimators with different properties. Furthermore, it is a common practice to correct the SDM effect and variance using the Hedges's correction [@Hedges1989-ip]. \n\nYou can directly implement another equation for the sampling variance or the Hedges's correction directly in the simulation function.\n\n## Simulating a single study - Pearson $\\rho$\n\nAnother common effect size is the Pearson correlation coefficient $\\rho$ (and the estimate $r$, see @eq-correlation). The variance of the correlation is calculated as:\n\n$$\nV_{r} = \\frac{(1 - r^2)^2}{n - 1}\n$$\n\n## Simulating a single study - Pearson $\\rho$\n\nThere is a huge dependency between $r$ and it's sampling variance (similar to the Cohen's $d$):\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nn <- 50\nr <- seq(0, 1, 0.01)\nv <- (1 - r^2)^2 / (n - 1) \n\nplot(r, v, type = \"l\", main = \"N = 50\", xlab = \"r\", ylab = latex2exp::TeX(\"$V_r$\"))\n```\n\n::: {.cell-output-display}\n![](intrometa_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\n\n## Simulating a single study - Pearson $\\rho$\n\nFor this reason the so-called Fisher's $z$ transformation is used to stabilize the relationship.\n\n$$\nz = \\frac{\\log{\\frac{1 + r}{1 - r}}}{2}\n$$\n\n$$\nV_z = \\frac{1}{n - 3}\n$$\n\nNow the variance is completely independent from the correlation value.\n\n## Simulating a single study - Pearson $\\rho$\n\nThis is the relationship between $r$ and $z$:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn <- 50\nr <- seq(-1, 1, 0.01)\nv <- (1 - r^2)^2 / (n - 1) \nz <- log((1 + r)/(1 - r))/2\n\nplot(z, r, type = \"l\", xlab = \"Fisher's z\", ylab = \"Correlation\", main = \"Correlation to Fisher's z\")\n```\n\n::: {.cell-output-display}\n![](intrometa_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\n\n## Simulating a single study - Pearson $\\rho$\n\nTo simulate a study using correlations we can use the `MASS::mvrnorm()` function that can generate correlated data from a multivariate normal distribution.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsim_r <- function(n, r){\n  R <- r + diag(1 - r, nrow = 2) # 2 x 2 correlation matrix\n  X <- MASS::mvrnorm(n, mu = c(0, 0), Sigma = R) # the means are not relevant here\n  r <- cor(X)[1, 2] # extract correlation\n  vr <- (1 - r^2)^2 / (n - 1)  # variance of r\n  yi <- log((1 + r)/(1 - r))/2 # fisher z\n  vi <- 1 / (n - 3) # fisher z variance\n  data.frame(yi, vi, r, vr) # including also the pearson correlation and variance\n}\n```\n:::\n\n\n\n## Simulating a single study - Pearson $\\rho$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsim_r(100, 0.5)\n#>          yi         vi         r         vr\n#> 1 0.6344433 0.01030928 0.5611042 0.00474189\nsim_r(50, 0.8)\n#>         yi        vi         r          vr\n#> 1 1.084392 0.0212766 0.7948222 0.002767626\n\n# also here the sampling distributions\nstudies <- replicate(1000, sim_r(50, 0.7), simplify = FALSE)\nstudies <- do.call(rbind, studies)\nsummary(studies)\n#>        yi               vi                r                vr           \n#>  Min.   :0.3773   Min.   :0.02128   Min.   :0.3603   Min.   :0.0007657  \n#>  1st Qu.:0.7743   1st Qu.:0.02128   1st Qu.:0.6494   1st Qu.:0.0040502  \n#>  Median :0.8719   Median :0.02128   Median :0.7024   Median :0.0052396  \n#>  Mean   :0.8697   Mean   :0.02128   Mean   :0.6942   Mean   :0.0055649  \n#>  3rd Qu.:0.9609   3rd Qu.:0.02128   3rd Qu.:0.7447   3rd Qu.:0.0068235  \n#>  Max.   :1.4615   Max.   :0.02128   Max.   :0.8979   Max.   :0.0154528\n```\n:::\n\n\n\n## More on effect sizes\n\nThe same logic can be applied to any situation. Just understand the data generation process, find the effect size equations and generate data.\n\n- @Borenstein2009-mo for all effect sizes equations. Also with equations to convert among effect sizes (useful in real-world meta-analyses)   \n- the [`metafor::escalc()`](https://wviechtb.github.io/metafor/reference/escalc.html) function implements basically any effect size. You can see also the [source code](https://github.com/wviechtb/metafor/blob/master/R/escalc.r) to see the actual R implementation.\n- [Guide to effect sizes](https://matthewbjane.quarto.pub/guide-to-effect-sizes-and-confidence-intervals): a modern and complete overview of effect sizes\n",
    "supporting": [
      "intrometa_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}