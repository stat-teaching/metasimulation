{
  "hash": "cef217f70c6d071f7c99acf2270b91c2",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Power analysis\"\n---\n\n\n\n\n\n\n# Statistical Power {.section}\n\n## Power in a nutshell^[Thanks to https://rpsychologist.com/creating-a-typical-textbook-illustration-of-statistical-power-using-either-ggplot-or-base-graphics]\n\nThe stastistical power is defined as the probability of correctly rejecting the null hypothesis $H_0$.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](power-analysis_files/figure-revealjs/unnamed-chunk-3-1.png){width=960}\n:::\n:::\n\n\n## Power in a nutshell\n\nFor simple designs such as t-test, ANOVA, etc. the power can be computed analytically. For example, let's find the power of detecting an effect size of $d = 0.5$ with $n1 = n2 = 30$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd <- 0.5\nalpha <- 0.05\nn1 <- n2 <- 30\nsp <- 1\n\n# Calculate non-centrality parameter (delta)\ndelta <- d * sqrt(n1 * n2 / (n1 + n2))\n\n# Calculate degrees of freedom\ndf <- n1 + n2 - 2\n\n# Calculate critical t-value\ncritical_t <- qt(1 - alpha / 2, df)\n\n# Calculate non-central t-distribution value\nnon_central_t <- delta / sp\n\n# Calculate power\n1 - pt(critical_t - non_central_t, df)\n## [1] 0.4741093\n```\n:::\n\n\n## Power in a nutshell\n\nThe same can be done using the `pwr` package:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npower <- pwr::pwr.t.test(n = n1, d = 0.5)\npower\n## \n##      Two-sample t test power calculation \n## \n##               n = 30\n##               d = 0.5\n##       sig.level = 0.05\n##           power = 0.4778965\n##     alternative = two.sided\n## \n## NOTE: n is number in *each* group\n```\n:::\n\n\n## Power in a nutshell\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](power-analysis_files/figure-revealjs/unnamed-chunk-6-1.png){width=960}\n:::\n:::\n\n\n## Power by simulations\n\nSometimes the analytical solution is not available or we can estimate the power for complex scenarios (missing data, unequal variances, etc.). The general workflow is:\n\n1. Generate data under the parametric assumptions\n2. Fit the appropriate model\n3. Extract the relevant metric (e.g., p-value)\n4. Repeat 1-3 several times (1000, 10000 or more)\n5. Summarise the results\n\nFor example, the power is the number of p-values lower than $\\alpha$ over the total number of simulations.\n\n## Power by simulations\n\nLet's see the previous example using simulations:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnsim <- 5000\np <- rep(NA, nsim)\nfor(i in 1:nsim){\n    g1 <- rnorm(n1, 0, 1)\n    g2 <- rnorm(n2, d, 1)\n    p[i] <- t.test(g1, g2)$p.value\n}\nmean(p <= alpha)\n## [1] 0.479\n```\n:::\n\n\nThe estimated value is pretty close to the analytical value.\n\n## What about meta-analysis\n\nAlso for meta-analysis we have the two approaches analytical and simulation-based.\n\n- The analytical approach for (intercept-only) random-effects and equal-effects model can be found on @Borenstein2009-mo (Chapter 29). See also @Valentine2010-aj and @Hedges2001-ra\n- @Jackson2017-dv proposed a similar but improved approach\n\n## Analytical approach\n\nFor the analytical approach we need to make some assumptions:\n\n- $\\tau^2$ and $\\mu_{\\theta}$ (or $\\theta$) are estimated without errors\n- The $\\sigma^2_i$ (thus the sample size) of each $k$ study is the same\n\nUnder these assumptions the power is:\n\n$$\n(1 - \\Phi(c_{\\alpha} - \\lambda)) + \\Phi(-c_{\\alpha} - \\lambda)\n$$\n\nWhere $c_{\\alpha}$ is the critical $z$ value and $\\lambda$ is the observed statistics.\n\n## Analytical approach - EE model\n\nFor an EE model the only source of variability is the sampling variability, thus $\\lambda$:\n\n$$\n\\lambda_{EE} = \\frac{\\theta}{\\sqrt{\\sigma^2_{\\theta}}}\n$$\n\nAnd recalling previous assuptions where $\\sigma^2_1 = \\dots = \\sigma^2_k$:\n\n$$\n\\sigma^2_{\\theta} = \\frac{\\sigma^2}{k}\n$$\n\n## Analytical approach - EE model\n\nFor example, a meta-analysis of $k = 15$ studies where each study have a sample size of $n1 = n2 = 20$ (assuming again unstandardized mean difference as effect size):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nk <- 10\ntheta <- 0.3\nn1 <- n2 <- 25\nvt <- 1/n1 + 1/n2\nvtheta <- vt/k\nlambda <- theta/sqrt(vtheta)\nzcrit <- abs(qnorm(0.05/2))\n(1 - pnorm(zcrit - lambda)) + pnorm(-zcrit - lambda)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9183621\n```\n\n\n:::\n:::\n\n\nBe careful that the EE model is assuming $\\tau^2 = 0$ thus is like having a huge study with $k \\times n_1$ participants per group.\n\n## Analytical approach - RE model\n\nFor the RE model we just need to include $\\tau^2$ in the $\\lambda$ calculation, thus:\n\n$$\n\\sigma^{2\\star}_{\\theta} = \\frac{\\sigma^2 + \\tau^2}{k}\n$$\n\nThe other calculations are the same as the EE model.\n\n## Analytical approach - RE model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nk <- 10\nmu <- 0.3\ntau2 <- 0.1\nn1 <- n2 <- 25\nvt <- 1/n1 + 1/n2\nvtheta <- (vt + tau2)/k\nlambda <- mu/sqrt(vtheta)\nzcrit <- abs(qnorm(0.05/2))\n(1 - pnorm(zcrit - lambda)) + pnorm(-zcrit - lambda)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.6087795\n```\n\n\n:::\n:::\n\n\nThe power is reduced because we are considering another source of heterogeneity. Clearly the maximal power of $k$ studies is achieved when $\\tau^2 = 0$. Hypothetically we can increase the power either increasing $k$ (the number of studies) or reducing $\\sigma^2_k$ (increasing the number of participants in each study).\n\n## Analytical approach - Power curves\n\nThe most informative approach is plotting the power curves for different values of $\\tau^2$, $\\sigma^2_k$ and $\\theta$ (or $\\mu_{\\theta}$).\n\nYou can use the `power_meta()` function:\n\n\n```r\n```\n\n\n## Analytical approach - Power curves\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](power-analysis_files/figure-revealjs/unnamed-chunk-11-1.png){width=960}\n:::\n:::\n\n\n## Analytical approach - Power curves\n\nWith the analytical approach we can (quickly) do interesting stuff. For example, we fix the total $N = n_1 + n_2$ for a series of $k$ and check the power.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](power-analysis_files/figure-revealjs/unnamed-chunk-12-1.png){width=960}\n:::\n:::\n\n\nAs long as $\\tau^2 \\neq 0$ we need more studies (even if the total sample size is the same).\n\n## Simulation-based power\n\nWith simulations we can fix or relax the previous assumptions. For example, let's compute the power for an EE model:\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.916\n```\n\n\n:::\n:::\n\n\nThe value is similar to the analytical simulation. But we can improve it e.g. generating heterogeneous sample sizes.\n\n## Simulation-based power curve\n\nBy repeating the previous approach for a series of parameters we can easily draw a power curve:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nk <- c(5, 10, 50, 100)\nes <- 0.1\ntau2 <- c(0, 0.05, 0.1, 0.2)\nnsim <- 1000\n\ngrid <- expand_grid(k, es, tau2)\npower <- rep(NA, nrow(grid))\n\nfor(i in 1:nrow(grid)){\n  pval <- rep(NA, nsim)\n  for(j in 1:nsim){\n    n <- rpois(grid$k[i], 40)\n    dat <- sim_studies(grid$k[i], grid$es[i], grid$tau2[i], n)\n    fit <- rma(yi, vi, data = dat)\n    pval[j] <- fit$pval\n  }\n  power[i] <- mean(pval <= 0.05)\n}\ngrid$power <- power\n```\n:::\n\n\n## Simulation-based power curve\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](power-analysis_files/figure-revealjs/unnamed-chunk-15-1.png){width=960}\n:::\n:::\n\n\n## Power analysis for meta-regression\n\nThe power for a meta-regression can be easily computed by simulating the moderator effect. For example, let's simulate the effect of a binary predictor $x$.\n\n\n::: {.cell}\n\n:::\n\n\n## Power analysis for meta-regression\n\nThen we can plot the results:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](power-analysis_files/figure-revealjs/unnamed-chunk-17-1.png){width=960}\n:::\n:::\n\n\n# Multilab Studies {.section}\n\n## Multilab studies\n\nMultilab studies can be seen as a meta-analysis that is planned (a *prospective* meta-analysis) compared to standard *retrospective* meta-analysis.\n\nThe statistical approach is (roughly) the same with the difference that we have control both on $k$ (the number of experimental units) and $n$ the sample size within each unit.\n\nIn multilab studies we have also the raw data (i.e., participant-level data) thus we can do more complex multilevel modeling.\n\n## Meta-analysis as multilevel model\n\nAssuming that we have $k$ studies with raw data available there is no need to aggregate, calculate the effect size and variances and then use an EE or RE model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nk <- 50\nes <- 0.4\ntau2 <- 0.1\nn <- round(runif(k, 10, 100))\ndat <- vector(mode = \"list\", k)\nthetai <- rnorm(k, 0, sqrt(tau2))\n\nfor(i in 1:k){\n  g1 <- rnorm(n[i], 0, 1)\n  g2 <- rnorm(n[i], es + thetai[i], 1)\n  d <- data.frame(id = 1:(n[i]*2), unit = i, y = c(g1, g2), group = rep(c(0, 1), each = n[i]))\n  dat[[i]] <- d\n}\n\ndat <- do.call(rbind, dat)\nht(dat)\n##      id unit          y group\n## 1     1    1 -0.3527787     0\n## 2     2    1  0.9250724     0\n## 3     3    1  0.5655600     0\n## 4     4    1  0.2058254     0\n## 5     5    1 -0.3201426     0\n## 5067 61   50 -0.1537464     1\n## 5068 62   50  2.1569767     1\n## 5069 63   50 -0.3760206     1\n## 5070 64   50  0.2661011     1\n## 5071 65   50  0.9310069     1\n## 5072 66   50 -0.5790200     1\n```\n:::\n\n\n## Meta-analysis as multilevel model\n\nThis is a simple **multilevel model** (pupils within classrooms or trials within participants). We can fit the model using `lme4::lmer()`:\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\nLinear mixed model fit by REML ['lmerMod']\nFormula: y ~ group + (1 | unit)\n   Data: dat\n\nREML criterion at convergence: 14915.8\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.4863 -0.6782  0.0043  0.6833  3.5032 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n unit     (Intercept) 0.04175  0.2043  \n Residual             1.09017  1.0441  \nNumber of obs: 5072, groups:  unit, 50\n\nFixed effects:\n             Estimate Std. Error t value\n(Intercept) -0.009541   0.036159  -0.264\ngroup        0.428598   0.029322  14.617\n\nCorrelation of Fixed Effects:\n      (Intr)\ngroup -0.405\n```\n\n\n:::\n:::\n\n\n## Meta-analysis as multilevel model\n\nLet's do the same as a meta-analysis. Firstly we compute the effect sizes for each unit:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndatagg <- dat |> \n  group_by(unit, group) |> \n  summarise(m = mean(y),\n            sd = sd(y),\n            n = n()) |> \n  pivot_wider(names_from = group, values_from = c(m, sd, n), names_sep = \"\")\n\ndatagg <- escalc(\"MD\", m1i = m1, m2i = m0, sd1i = sd1, sd2i = sd0, n1i = n1, n2i = n0, data = datagg)\n```\n:::\n\n\n## Meta-analysis as multilevel model\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n\n   unit           m0           m1       sd0       sd1 n0 n1      yi     vi \n1     1  0.049965709 -0.004962583 0.9373755 1.0510537 63 63 -0.0549 0.0315 \n2     2  0.090771638  0.485499771 0.9893023 0.9523611 78 78  0.3947 0.0242 \n3     3  0.128519687  0.283088771 1.0111578 0.8513269 53 53  0.1546 0.0330 \n4     4  0.119270070 -0.458742812 1.0337232 1.0412068 57 57 -0.5780 0.0378 \n5     5 -0.118120850  1.150522996 0.9977532 1.3457892 37 37  1.2686 0.0759 \n45   45  0.057973305 -0.026451787 1.0597983 0.9950742 26 26 -0.0844 0.0813 \n46   46 -0.091659519  0.622772286 1.0599128 0.7928472 59 59  0.7144 0.0297 \n47   47  0.008253221 -0.109140947 0.8923834 0.9596149 70 70 -0.1174 0.0245 \n48   48  0.011306010  0.098236489 1.0257345 1.0498893 45 45  0.0869 0.0479 \n49   49 -0.042973407  0.632997457 1.3417626 0.9962859 16 16  0.6760 0.1746 \n50   50 -0.063252249  0.378438493 0.8917357 0.9194668 33 33  0.4417 0.0497 \n```\n\n\n:::\n:::\n\n\n## Meta-analysis as multilevel model\n\nThen we can fit the model:\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n\nRandom-Effects Model (k = 50; tau^2 estimator: REML)\n\ntau^2 (estimated amount of total heterogeneity): 0.1803 (SE = 0.0462)\ntau (square root of estimated tau^2 value):      0.4246\nI^2 (total heterogeneity / total variability):   81.75%\nH^2 (total variability / sampling variability):  5.48\n\nTest for Heterogeneity:\nQ(df = 49) = 288.5404, p-val < .0001\n\nModel Results:\n\nestimate      se    zval    pval   ci.lb   ci.ub      \n  0.4185  0.0679  6.1625  <.0001  0.2854  0.5516  *** \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n## Meta-analysis as multilevel modeling\n\nActually the results are very similar where the standard deviation of the intercepts of the `lme4` model is $\\approx \\tau$ and the `group` effect is the intercept of the `rma` model.\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n              b         se       tau2   model\ngroup 0.4285978 0.02932156 0.04174759    lme4\n      0.4184767 0.06790661 0.18025386 metafor\n```\n\n\n:::\n:::\n\n\nActually the two model are not exactly the same, especially when using only the aggregated data. See https://www.metafor-project.org/doku.php/tips:rma_vs_lm_lme_lmer.\n\n## Meta-analysis as multilevel modeling\n\nTo note, aggregating data and then computing a standard (non-weighted) model (sometimes this is done with trial-level data) is wrong and should be avoided. Using meta-analysis is clear that aggregating without taking into account the cluster (e.g., study or subject) precision is misleading.\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\nLinear mixed model fit by REML ['lmerMod']\nFormula: y ~ group + (1 | unit)\n   Data: dataggl\n\nREML criterion at convergence: 69\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.83089 -0.44445 -0.00956  0.38240  2.92162 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n unit     (Intercept) 0.0000   0.0000  \n Residual             0.1093   0.3305  \nNumber of obs: 100, groups:  unit, 50\n\nFixed effects:\n             Estimate Std. Error t value\n(Intercept) -0.004819   0.046746  -0.103\ngroupm1      0.411316   0.066109   6.222\n\nCorrelation of Fixed Effects:\n        (Intr)\ngroupm1 -0.707\noptimizer (nloptwrap) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')\n```\n\n\n:::\n:::\n\n\n## Mulitlab sample size vs unit\n\nWhen planning a multilab study there is an important decision between **increasing the sample size within each unit** (more effort for each lab) or **recruiting more units** with less participants per unit (more effort for the organization).\n\nWe could have the situation where the number of units $k$ is fixed and we can only increase the sample size.\n\nWe can also simulate scenarios where some units collect all data while others did not complete the data collection.\n\n## Fixed $k$, increasing $n$\n\nLet's assume that the maximum number of labs is $10$. How many participants are required assuming a certain amount of heterogeneity?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nes <- 0.2\nk <- 10\nn1 <- n2 <- seq(10, 500, 10)\ntau2 <- c(0.01, 0.05, 0.1, 0.2)\nsim <- expand_grid(k, es, tau2, n1)\nsim$n2 <- sim$n1\nsim$vt <- with(sim, 1/n1 + 1/n2)\nsim$I2 <- round(with(sim, tau2 / (tau2 + vt)) * 100, 3)\nsim$power <- power_meta(sim$es, sim$k, sim$tau2, sim$n1, sim$n2)\n\nht(sim)\n## # A tibble: 11 × 8\n##        k    es  tau2    n1    n2      vt    I2 power\n##    <dbl> <dbl> <dbl> <dbl> <dbl>   <dbl> <dbl> <dbl>\n##  1    10   0.2  0.01    10    10 0.2      4.76 0.281\n##  2    10   0.2  0.01    20    20 0.1      9.09 0.479\n##  3    10   0.2  0.01    30    30 0.0667  13.0  0.627\n##  4    10   0.2  0.01    40    40 0.05    16.7  0.733\n##  5    10   0.2  0.01    50    50 0.04    20    0.807\n##  6    10   0.2  0.2    450   450 0.00444 97.8  0.288\n##  7    10   0.2  0.2    460   460 0.00435 97.9  0.288\n##  8    10   0.2  0.2    470   470 0.00426 97.9  0.288\n##  9    10   0.2  0.2    480   480 0.00417 98.0  0.288\n## 10    10   0.2  0.2    490   490 0.00408 98    0.288\n## 11    10   0.2  0.2    500   500 0.004   98.0  0.288\n```\n:::\n\n\n## Fixed $k$, increasing $n$\n\nWith a fixed $k$, we could reach a plateau even increasing $n$. This depends also on $\\mu_{\\theta}$ and $\\tau^2$.\n\n::: {.cell}\n::: {.cell-output-display}\n![](power-analysis_files/figure-revealjs/unnamed-chunk-26-1.png){width=960}\n:::\n:::\n\n\n## Multilab replication studies\n\nA special type of multilab studies are the replication projects. There are some paper discussing how to view replication studies as meta-analyses and how to plan them.\n\n- @Hedges2021-of\n- @Schauer2022-mj\n- @Schauer2020-tw\n- @Schauer2021-ja\n- @Schauer2023-yn\n- @Hedges2019-ry\n\n## References\n\n::: {#refs}\n:::",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}