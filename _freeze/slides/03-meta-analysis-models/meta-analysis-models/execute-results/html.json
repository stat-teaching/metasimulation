{
  "hash": "b32a3508a95c3ae30f3e66cf314c2289",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Meta-analysis Models\nsubtitle: CSA 2024\n---\n\n\n\n\n\n\n# Simulation setup {.section}\n\n## Notation {.smaller}\n\nMeta-analysis notation is a little bit inconsistent in textbooks and papers. We define here some rules to simplify the work.\n\n- $k$ is the number of studies\n- $n_j$ is the sample size of the group $j$ within a study\n- $y_i$ are the observed effect size included in the meta-analysis\n- $\\sigma_i^2$ are the observed sampling variance of studies and $\\epsilon_i$ are the sampling errors\n- $\\theta$ is the equal-effects parameter (see @eq-ee1)\n- $\\delta_i$ is the random-effect (see @eq-re-mod2)\n- $\\mu_\\theta$ is the average effect of a random-effects model (see @eq-re-mod1)\n- $w_i$ are the meta-analysis weights\n- $\\tau^2$ is the heterogeneity (see @eq-re-mod2)\n- $\\Delta$ is the (generic) population effect size\n- $s_j^2$ is the variance of the group $j$ within a study\n\n## Simulation setup\n\nGiven the introduction to effect sizes, from now we will simulate data using UMD and the individual-level data. \n\nBasically we are simulating an effect size $D$ coming from the comparison of two independent groups $G_1$ and $G_2$.\n\nEach group is composed by $n$ participants measured on a numerical outcome (e.g., reaction times)\n\n## Simulation setup\n\nA more general, clear and realistic approach to simulate data is by generating $k$ studies with same/different sample sizes and (later) true effect sizes.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nk <- 10 # number of studies\nn1 <- n2 <- 10 + rpois(k, 30 - 10) # sample size from poisson distribution with lambda 40 and minimum 10\nD <- 0.5 # effect size\n\nyi <- rep(NA, k)\nvi <- rep(NA, k)\n  \nfor(i in 1:k){\n  g1 <- rnorm(n1[i], 0, 1)\n  g2 <- rnorm(n2[i], D, 1)\n  yi[i] <- mean(g2) - mean(g1)\n  vi[i] <- var(g1)/n1[i] + var(g2)/n2[i]\n}\n  \nsim <- data.frame(id = 1:k, yi, vi)\n\nhead(sim)\n#>   id         yi         vi\n#> 1  1  0.4378059 0.03923595\n#> 2  2  0.9297779 0.05841354\n#> 3  3 -0.1313321 0.07501271\n#> 4  4  0.3465024 0.09547210\n#> 5  5  0.5029728 0.05942748\n#> 6  6  0.4497768 0.08466673\n```\n:::\n\n\n## Simulation setup\n\nWe can again put everything within a function:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsim_studies <- function(k, es, n1, n2 = NULL){\n  if(length(n1) == 1) n1 <- rep(n1, k)\n  if(is.null(n2)) n2 <- n1\n  if(length(es) == 1) es <- rep(es, k)\n  \n  yi <- rep(NA, k)\n  vi <- rep(NA, k)\n  \n  for(i in 1:k){\n    g1 <- rnorm(n1[i], 0, 1)\n    g2 <- rnorm(n2[i], es[i], 1)\n    yi[i] <- mean(g2) - mean(g1)\n    vi[i] <- var(g1)/n1[i] + var(g2)/n2[i]\n  }\n  \n  sim <- data.frame(id = 1:k, yi, vi, n1 = n1, n2 = n2)\n  \n  # convert to escalc for using metafor methods\n  sim <- metafor::escalc(yi = yi, vi = vi, data = sim)\n  \n  return(sim)\n}\n```\n:::\n\n\n## Simulation setup - Disclaimer\n\nThe proposed simulation approach using a `for` loop and separated vectors. For the purposed of the workshop this is the best option. In real-world meta-analysis simulations you can choose a more functional approach starting from a simulation grid as `data.frame` and mapping the simulation functions.\n\nFor some examples see:\n\n- @Gambarota2023-on\n- [www.jepusto.com/simulating-correlated-smds](https://www.jepusto.com/simulating-correlated-smds)\n\n# Combining studies {.section}\n\n## Combining studies\n\nLet's imagine to have $k = 10$ studies, a $D = 0.5$ and heterogeneous sample sizes in each study.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nk <- 10\nD <- 0.5\nn <- 10 + rpois(k, lambda = 20) \ndat <- sim_studies(k = k, es = D, n1 = n)\nhead(dat)\n#> \n#>   id     yi     vi n1 n2 \n#> 1  1 0.8319 0.0734 29 29 \n#> 2  2 0.5009 0.0501 28 28 \n#> 3  3 0.3937 0.0560 33 33 \n#> 4  4 0.4665 0.0823 22 22 \n#> 5  5 0.2819 0.0754 21 21 \n#> 6  6 1.2067 0.0951 27 27\n```\n:::\n\n\n. . .\n\nWhat is the best way to combine the studies?\n\n## Combining studies\n\nWe can take the average effect size and considering it as a huge study. This can be considered the best way to combine the effects.\n\n$$\n\\hat{D} = \\frac{\\sum^{k}_{i = 1} D_i}{k}\n$$\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmean(dat$yi)\n#> [1] 0.5672026\n```\n:::\n\n\n. . .\n\nIt is appropriate? What do you think? Are we missing something?\n\n## Weighting studies\n\nWe are not considering that some studies, despite providing a similar effect size could give more information. An higher sample size (or lower sampling variance) produce a more reliable estimation.\n\n. . .\n\nWould you trust more a study with $n = 100$ and $D = 0.5$ or a study with $n = 10$ and $D = 0.5$? The \"meta-analysis\" that we did before is completely ignoring this information.\n\n## Weighting studies\n\nWe need to find a value (called weight $w_i$) that allows assigning more trust to a study because it provide more information. \n\n. . .\n\nThe simplest weights are just the sample size, but in practice we use the so-called **inverse-variance weighting**. We use the (inverse) of the sampling variance of the effect size to weight each study. \n\n. . .\n\nThe basic version of a meta-analysis is just a **weighted average**:\n\n$$\n\\overline D_w = \\frac{\\sum^k_{i = 1}{w_iD_i}}{\\sum^k_{i = 1}{w_i}}\n$$\n\n. . .\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nwi <- 1/dat$vi\nsum(dat$yi * wi) / sum(wi)\n#> [1] 0.5473616\n# weighted.mean(dat$yi, wi)\n```\n:::\n\n\n## Weighting studies\n\nGraphically, the two models can be represented in this way:\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](meta-analysis-models_files/figure-revealjs/unnamed-chunk-8-1.svg){fig-align='center' width=960}\n:::\n:::\n\n\n# Equal-effects (EE) meta-analysis {.section}\n\n## EE meta-analysis\n\nWhat we did in the last example (the weighted mean) is the exactly a meta-analysis model called **equal-effects** (or less precisely fixed-effect). The assumptions are very simple:\n\n- there is a unique, true effect size to estimate $\\theta$\n- each study is a more or less precise estimate of $\\theta$\n- there is no TRUE variability among studies. The observed variability is due to studies that are imprecise (i.e., sampling error)\n- assuming that each study has a very large sample size, the observed variability is close to zero.\n\n## EE meta-analysis, formally\n\n$$\ny_i = \\theta + \\epsilon_i\n$$ {#eq-ee1}\n\n$$\n\\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2_i)\n$$ {#eq-ee2}\n\nWhere $\\sigma^2_i$ is the vector of sampling variabilities of $k$ studies. This is a standard linear model but with heterogeneous sampling variances.\n\n## EE meta-analysis\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](meta-analysis-models_files/figure-revealjs/unnamed-chunk-9-1.svg){fig-align='center' width=960}\n:::\n:::\n\n\n## Simulating an EE model\n\nWhat we were doing with the `sim_studies()` function so far was simulating an EE model. In fact, there were a single $\\theta$ parameter and the observed variability was a function of the `rnorm()` randomness.\n\nBased on previous assumptions and thinking a little bit, what could be the result of simulating studies with a very large $n$?\n\n. . .\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nns <- c(10, 50, 100, 1000, 1e4)\nD <- 0.5\ndats <- lapply(ns, function(n) sim_studies(10, es = D, n1 = n))\n```\n:::\n\n\n## Simulating an EE modelm {#sec-ee-impact-n}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](meta-analysis-models_files/figure-revealjs/unnamed-chunk-11-1.svg){fig-align='center' width=960}\n:::\n:::\n\n\n## Simulating an EE model\n\nFormulating the model as a intercept-only regression (see Equations [@eq-ee1] and [@eq-ee2]) we can generate data directly:\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\nAs we did for the aggregated data approach. Clearly we need to simulate also the `vi` vector from the appropriate distribution. Given that we simulated data starting from the participant-level the uncertainty of `yi` and `vi` is already included.\n\n## Fitting an EE model\n\nThe model can be fitted using the `metafor::rma()` function, with `method = \"EE\"`^[There is a confusion about the *fixed-effects* vs *fixed-effect* (no *s*) and *equal-effects* models. See [https://wviechtb.github.io/metafor/reference/misc-models.html](https://wviechtb.github.io/metafor/reference/misc-models.html)].\n\n\n::: {.cell layout-align=\"center\"}\n\n```\n#> \n#> Equal-Effects Model (k = 15)\n#> \n#>   logLik  deviance       AIC       BIC      AICc   \n#>  -1.8216   15.3742    5.6432    6.3513    5.9509   \n#> \n#> I^2 (total heterogeneity / total variability):   8.94%\n#> H^2 (total variability / sampling variability):  1.10\n#> \n#> Test for Heterogeneity:\n#> Q(df = 14) = 15.3742, p-val = 0.3531\n#> \n#> Model Results:\n#> \n#> estimate      se    zval    pval   ci.lb   ci.ub      \n#>   0.3798  0.0684  5.5537  <.0001  0.2458  0.5138  *** \n#> \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n\n\n## Interpreting an EE model\n\n- The first section (`logLik`, `deviance`, etc.) presents some general model statistics and information criteria\n- The $I^2$ and $H^2$ are statistics evaluating the observed heterogeneity (see next slides)\n- The `Test of Heterogeneity` section presents the test of the $Q$ statistics for the observed heterogeneity (see next slides)\n- The `Model Results` section presents the estimation of the $\\theta$ parameter along with the standard error and the Wald $z$ test ($H_0: \\theta = 0$)\n\nThe `metafor` package has a several well documented functions to calculate and plot model results, residuals analysis etc.\n\n## Interpreting an EE model\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](meta-analysis-models_files/figure-revealjs/unnamed-chunk-14-1.svg){fig-align='center' width=960}\n:::\n:::\n\n\n## Interpreting an EE Model\n\nThe main function for plotting model results is the `forest()` function that produce the forest plot.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](meta-analysis-models_files/figure-revealjs/unnamed-chunk-15-1.svg){fig-align='center' width=960}\n:::\n:::\n\n\n## Interpreting an EE Model\n\nWe did not introduced the concept of heterogeneity, but the $I^2$, $H^2$ and $Q$ statistics basically evaluate if the observed heterogeneity should be attributed to **sampling variability** (uncertainty in estimating $\\theta$ because we have a limited $k$ and $n$) or **sampling variability** plus other sources of heterogeneity.\n\n## EE model as a weighted Average\n\nFormally $\\theta$ is estimated as [see @Borenstein2009-mo, p. 66]\n\n$$\n\\hat{\\theta} = \\frac{\\sum^k_{i = 1}{w_iy_i}}{\\sum^k_{i = 1}{w_i}}; \\;\\;\\; w_i = \\frac{1}{\\sigma^2_i}\n$$\n\n$$\nSE_{\\theta} = \\frac{1}{\\sum^k_{i = 1}{w_i}}\n$$\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nwi <- 1/dat$vi\ntheta_hat <- with(dat, sum(yi * wi)/sum(wi))\nse_theta_hat <- sqrt(1/sum(wi))\nc(theta = theta_hat, se = se_theta_hat, z = theta_hat / se_theta_hat)\n#>      theta         se          z \n#> 0.37977974 0.06838334 5.55368810\n```\n:::\n\n\n# Random-effects (RE) meta-analysis {.section}\n\n## Are the EE assumptions realistic?\n\nThe EE model is appropriate if our studies are somehow **exact replications** of the exact same effect. We are assuming that there is **no real variability**.\n\n. . .\n\nHowever, meta-analysis rarely report the results of $k$ exact replicates. It is more common to include **studies answering the same research question** but with different methods, participants, etc.\n\n. . .\n\n- people with different ages or other participant-level differences\n- different methodology\n- ...\n\n## Are the EE assumptions realistic?\n\n. . .\n\nIf we relax the previous assumption we are able to combine studies that are not exact replications. \n\n. . .\n\nThus the real effect $\\theta$ is no longer a single **true** value but can be larger or smaller in some conditions.\n\n. . .\n\nIn other terms we are assuming that there could be some variability (i.e., **heterogeneity**) among studies that is independent from the sample size. Even with studies with $\\lim_{n\\to\\infty}$ the observed variability is not zero.\n\n## Random-effects model (RE)\n\nWe can extend the EE model including another source of variability, $\\tau^2$. $\\tau^2$ is the true heterogeneity among studies caused by methdological differences or intrisic variability in the phenomenon.\n\nFormally we can extend @eq-ee1 as:\n$$\ny_i = \\mu_{\\theta} + \\delta_i + \\epsilon_i\n$$ {#eq-re-mod1}\n\n$$\n\\delta_i \\sim \\mathcal{N}(0, \\tau^2)\n$$ {#eq-re-mod2}\n\n$$\n\\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2_i)\n$$\n\nWhere $\\mu_{\\theta}$ is the average effect size and $\\delta_i$ is the study-specific deviation from the average effect (regulated by $\\tau^2$). Clearly each study specific effect is $\\theta_i = \\mu_{\\theta} + \\delta_i$.\n\n## RE model\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](meta-analysis-models_files/figure-revealjs/unnamed-chunk-17-1.svg){fig-align='center' width=60%}\n:::\n:::\n\n\n## RE model estimation\n\nGiven that we extended the EE model equation. Also the estimation of the average effect need to be extended. Basically the RE is still a weighted average but weights need to include also $\\tau^2$.\n\n$$\n\\overline y = \\frac{\\sum_{i = 1}^k y_iw^*_i}{\\sum_{i = 1}^k w^*_i}\n$$ {#eq-re1}\n\n$$\nw^*_i = \\frac{1}{\\sigma^2_i + \\tau^2}\n$$ {#eq-re2}\n\nThe weights are different compared to the EE model. Extremely precise/imprecise studies will have less impact in the RE model.\n\n## RE vs EE model\n\nThe crucial difference with the EE model is that even with large $n$, only the $\\mu_{\\theta} + \\delta_i$ are estimated (almost) without error. As long $\\tau^2 \\neq 0$ there will be variability in the effect sizes.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](meta-analysis-models_files/figure-revealjs/unnamed-chunk-18-1.svg){fig-align='center' width=960}\n:::\n:::\n\n\n## Simulating a RE Model\n\nTo simulate the RE model we simply need to include $\\tau^2$ in the EE model simulation.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nk <- 15 # number of studies\nmu <- 0.5 # average effect\ntau2 <- 0.1 # heterogeneity\nn <- 10 + rpois(k, 30 - 10) # sample size\ndeltai <- rnorm(k, 0, sqrt(tau2)) # random-effects\nthetai <- mu + deltai # true study effect\n\ndat <- sim_studies(k = k, es = thetai, n1 = n)\n\nhead(dat)\n#> \n#>   id      yi     vi n1 n2 \n#> 1  1  0.4744 0.0700 30 30 \n#> 2  2  0.5973 0.0587 32 32 \n#> 3  3  0.5678 0.0920 28 28 \n#> 4  4 -0.1278 0.0594 31 31 \n#> 5  5  0.5915 0.0780 33 33 \n#> 6  6  0.1417 0.0935 22 22\n```\n:::\n\n\n## Simulating a RE model\n\nAgain, we can put everything within a function expanding the previous `sim_studies()` by including $\\tau^2$:\n\n\n::: {.cell layout-align=\"center\"}\n```r\n```\n:::\n\n\n## Simulating a RE model\n\nThe data are similar to the EE simulation but we have an extra source of heterogeneity.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](meta-analysis-models_files/figure-revealjs/unnamed-chunk-21-1.svg){fig-align='center' width=960}\n:::\n:::\n\n\n## Simulating a RE model\n\nTo see the actual impact of $\\tau^2$ we can follow the same approach of @sec-ee-impact-n thus using a large $n$. The sampling variance `vi` of each study is basically 0.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# ... other parameters as before\nn <- 1e4\ndeltai <- rnorm(k, 0, sqrt(tau2)) # random-effects\nthetai <- mu + deltai # true study effect\ndat <- sim_studies(k = k, es = thetai, n1 = n)\n# or equivalently \n# dat <- sim_studies(k = k, es = mu, tau2 = tau2, n1 = n)\n\nhead(dat)\n#> \n#>   id      yi     vi    n1    n2 \n#> 1  1  0.1485 0.0002 10000 10000 \n#> 2  2  0.4897 0.0002 10000 10000 \n#> 3  3 -0.2400 0.0002 10000 10000 \n#> 4  4  0.2305 0.0002 10000 10000 \n#> 5  5  0.3771 0.0002 10000 10000 \n#> 6  6  0.4781 0.0002 10000 10000\n```\n:::\n\n\n## Simulating a RE Model\n\nClearly, compared to @sec-ee-impact-n, even with large $n$ the variability is not reduced because $\\tau^2 \\neq 0$. As $\\tau^2$ approach zero the EE and RE models are similar.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](meta-analysis-models_files/figure-revealjs/unnamed-chunk-23-1.svg){fig-align='center' width=960}\n:::\n:::\n\n\n## RE model estimation\n\nGiven that we extended the EE model equation. Also the estimation of the average effect need to be extended. Basically the RE is still a weighted average but weights need to include also $\\tau^2$.\n\n$$\n\\overline y = \\frac{\\sum_{i = 1}^k y_iw^*_i}{\\sum_{i = 1}^k w^*_i}\n$$ {#eq-re1}\n\n$$\nw^*_i = \\frac{1}{\\sigma^2_i + \\tau^2}\n$$ {#eq-re2}\n\nThe weights are different compared to the EE model. Extremely precise/imprecise studies will have less impact in the RE model.\n\n## Fitting a RE model\n\nIn R we can use the `metafor::rma()` function using the `method = \"REML\"`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```\n#> \n#> Random-Effects Model (k = 15; tau^2 estimator: REML)\n#> \n#>   logLik  deviance       AIC       BIC      AICc   \n#>  -0.5383    1.0766    5.0766    6.3547    6.1675   \n#> \n#> tau^2 (estimated amount of total heterogeneity): 0.0630 (SE = 0.0239)\n#> tau (square root of estimated tau^2 value):      0.2511\n#> I^2 (total heterogeneity / total variability):   99.68%\n#> H^2 (total variability / sampling variability):  316.39\n#> \n#> Test for Heterogeneity:\n#> Q(df = 14) = 4459.8553, p-val < .0001\n#> \n#> Model Results:\n#> \n#> estimate      se    zval    pval   ci.lb   ci.ub      \n#>   0.3879  0.0649  5.9752  <.0001  0.2607  0.5152  *** \n#> \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n\n\n## Intepreting the RE model\n\nThe model output is quite similar to the EE model and also the intepretation is similar.\n\nThe only extra section is `tau^2/tau` that is the estimation of the between-study heterogeneity.\n\n## Estimating $\\tau^2$\n\nWe used `method = \"REML\"` but there are actually several estimators for $\\tau^2$. @Viechtbauer2005-zt, @Veroniki2016-zs and @Brannick2019-hv provided a comprehensive overview of different estimators.\n\niframe hete\n\n## Estimating $\\tau^2$\n\nThe **Restricted Maximum Likelihood** (REML) estimator is considered one of the best. We can compare the results using the `all_rma()` custom function that tests all the estimators^[The `filor::compare_rma()` function is similar to the `car::compareCoefs()` function].\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfitl <- all_rma(fit)\nround(filor::compare_rma(fitlist = fitl), 3)\n#>           DL     HE     HS    HSk     SJ     ML   REML     EB     PM    PMM\n#> b      0.388  0.388  0.388  0.388  0.388  0.388  0.388  0.388  0.388  0.388\n#> se     0.065  0.065  0.063  0.065  0.065  0.063  0.065  0.065  0.065  0.067\n#> zval   5.955  5.975  6.164  5.955  5.976  6.185  5.975  5.975  5.975  5.832\n#> pval   0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n#> ci.lb  0.260  0.261  0.265  0.260  0.261  0.265  0.261  0.261  0.261  0.258\n#> ci.ub  0.516  0.515  0.511  0.516  0.515  0.511  0.515  0.515  0.515  0.518\n#> I2    99.686 99.684 99.664 99.686 99.684 99.661 99.684 99.684 99.684 99.699\n#> tau2   0.063  0.063  0.059  0.063  0.063  0.059  0.063  0.063  0.063  0.066\n```\n:::\n\n\n## Intepreting heterogeneity $\\tau^2$\n\nLooking at @eq-re-mod2, $\\tau^2$ is essentially the variance of the random-effect. This means that we can intepret it as the variability (or the standard deviation) of the true effect size distribution.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](meta-analysis-models_files/figure-revealjs/unnamed-chunk-26-1.svg){fig-align='center' width=960}\n:::\n:::\n\n\n## Intepreting $\\tau^2$\n\nAs in the previus plot we can assume $n = \\infty$ and generate true effects from @eq-re-mod2. In this way we understand the impact of assuming (or estimating) a certain $\\tau^2$.\n\nFor example, a $\\tau = 0.2$ and a $\\mu_{\\theta} = 0.5$, 50% of the true effects ranged between:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nD <- 0.5\nyis <- D + rnorm(1e5, 0, 0.2)\nquantile(yis, c(0.75, 0.25))\n#>       75%       25% \n#> 0.6363933 0.3663519\n```\n:::\n\n\n## The $Q$ Statistics^[See @Harrer2021-go (Chapter 5) and @Hedges2019-ry for an overview about the Q statistics]\n\nThe Q statistics is used to make inference on the heterogeneity. Can be considered as a weighted sum of squares:\n\n$$\nQ = \\sum^k_{i = 1}w_i(y_i - \\hat \\mu)^2\n$$\n\nWhere $\\hat \\mu$ is EE estimation (regardless if $\\tau^2 \\neq 0$) and $w_i$ are the inverse-variance weights. Note that in the case of $w_1 = w_2 ... = w_i$, Q is just a standard sum of squares (or deviance).\n\n## The $Q$ Statistics\n\n- Given that we are summing up squared distances, they should be approximately $\\chi^2$ with $df = k - 1$. In case of no heterogeneity ($\\tau^2 = 0$) the observed variability is only caused by sampling error and the expectd value of the $\\chi^2$ is just the degrees of freedom ($df = k - 1$).\n- In case of $\\tau^2 \\neq 0$, the expected value is $k - 1 + \\lambda$ where $\\lambda$ is a non-centrality parameter.\n- In other terms, if the expected value of $Q$ exceed the expected value assuming no heterogeneity, we have evidence that $\\tau^2 \\neq 0$.\n\n## The $Q$ Statistics\n\nLet's try a more practical approach. We simulate a lot of meta-analysis with and without heterogeneity and we check the Q statistics.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nget_Q <- function(yi, vi){\n  wi <- 1/vi\n  theta_ee <- weighted.mean(yi, wi)\n  sum(wi*(yi - theta_ee)^2)\n}\n\nk <- 30\nn <- 30\ntau2 <- 0.1\nnsim <- 1e4\n\nQs_tau2_0 <- rep(0, nsim)\nQs_tau2 <- rep(0, nsim)\nres2_tau2_0 <- vector(\"list\", nsim)\nres2_tau2 <- vector(\"list\", nsim)\n\nfor(i in 1:nsim){\n  dat_tau2_0 <- sim_studies(k = 30, es = 0.5, tau2 = 0, n1 = n)\n  dat_tau2 <- sim_studies(k = 30, es = 0.5, tau2 = tau2, n1 = n)\n  \n  theta_ee_tau2_0 <- weighted.mean(dat_tau2_0$yi, 1/dat_tau2_0$vi)\n  theta_ee <- weighted.mean(dat_tau2$yi, 1/dat_tau2$vi)\n  \n  res2_tau2_0[[i]] <- dat_tau2_0$yi - theta_ee_tau2_0\n  res2_tau2[[i]] <- dat_tau2$yi - theta_ee\n  \n  Qs_tau2_0[i] <- get_Q(dat_tau2_0$yi, dat_tau2_0$vi)\n  Qs_tau2[i] <- get_Q(dat_tau2$yi, dat_tau2$vi)\n}\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](meta-analysis-models_files/figure-revealjs/unnamed-chunk-29-1.svg){fig-align='center' width=960}\n:::\n:::\n\n\n## The $Q$ Statistics\n\nLet's try a more practical approach. We simulate a lot of meta-analysis with and without heterogeneity and we check the Q statistics.\n\n. . .\n\n- clearly, in the presence of heterogeneity, the expected value of the Q statistics is higher (due to $\\lambda \\neq 0$) and also residuals are larger (the $\\chi^2$ is just a sum of squared weighted residuals)\n\n. . .\n\n- we can calculate a p-value for deviation from the $\\tau^2 = 0$ case as evidence agaist the absence of heterogeneity\n\n## $I^2$ [@Higgins2002-fh]\n\nWe have two sources of variability in a random-effects meta-analysis, the sampling variability $\\sigma_i^2$ and true heterogeneity $\\tau^2$. We can use the $I^2$ to express the interplay between the two.\n$$\nI^2 = 100\\% \\times \\frac{\\hat{\\tau}^2}{\\hat{\\tau}^2 + \\tilde{v}}\n$${#eq-i2}\n\n$$\n\\tilde{v} = \\frac{(k-1) \\sum w_i}{(\\sum w_i)^2 - \\sum w_i^2},\n$$\n\nWhere $\\tilde{v}$ is the typical sampling variability. $I^2$ is intepreted as the proportion of total variability due to real heterogeneity (i.e., $\\tau^2$)\n\n## $I^2$ [@Higgins2002-fh]^[see [https://www.meta-analysis-workshops.com/download/common-mistakes1.pdf](https://www.meta-analysis-workshops.com/download/common-mistakes1.pdf)]\n\nNote that we can have the same $I^2$ in two completely different meta-analysis. An high $I^2$ does not represent high heterogeneity. Let's assume to have two meta-analysis with $k$ studies and small ($n = 30$) vs large ($n = 500$) sample sizes. \n\nLet's solve @eq-i2 for $\\tau^2$ (using `filor::tau2_from_I2()`) and we found that the same $I^2$ can be obtained with two completely different $\\tau^2$ values:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nn_1 <- 30\nvi_1 <- 1/n_1 + 1/n_1\ntau2_1 <- filor::tau2_from_I2(0.8, vi_1)\ntau2_1\n#> [1] 0.2666667\n\nn_2 <- 500\nvi_2 <- 1/n_2 + 1/n_2\ntau2_2 <- filor::tau2_from_I2(0.8, vi_2)\ntau2_2\n#> [1] 0.016\n```\n:::\n\n\n## $I^2$ [@Higgins2002-fh]\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nn_1 <- 30\nvi_1 <- 1/n_1 + 1/n_1\ntau2_1 <- filor::tau2_from_I2(0.8, vi_1)\ntau2_1\n#> [1] 0.2666667\n\nn_2 <- 500\nvi_2 <- 1/n_2 + 1/n_2\ntau2_2 <- filor::tau2_from_I2(0.8, vi_2)\ntau2_2\n#> [1] 0.016\n```\n:::\n\n\n. . .\n\nIn other terms, the $I^2$ can be considered a good index of heterogeneity only when the total variance ($\\tilde{v} + \\tau^2$) is similar.\n\n## What about $\\tilde{v}$?\n\n$\\tilde{v}$ is considered the \"typical\" within-study variability (see [https://www.metafor-project.org/doku.php/tips:i2_multilevel_multivariate](https://www.metafor-project.org/doku.php/tips:i2_multilevel_multivariate)). There are different estimators but @eq-tildev [@Higgins2002-fh] is the most common.\n\n$$\n\\tilde{v} = \\frac{(k-1) \\sum w_i}{(\\sum w_i)^2 - \\sum w_i^2}\n$$ {#eq-tildev}\n\n## What about $\\tilde{v}$?\n\nIn the hypothetical case where $\\sigma^2_1 = \\dots = \\sigma^2_k$, $\\tilde{v}$ is just $\\sigma^2$. This fact is commonly used to calculate the statistical power analytically [@Borenstein2009-mo, Chapter 29].\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nvtilde <- function(wi){\n  k <- length(wi)\n  (k - 1) * sum(wi) / (sum(wi)^2 - sum(wi^2))\n}\n\nk <- 20\n\n# same vi\nvi <- rep((1/30 + 1/30), k)\nhead(vi)\n#> [1] 0.06666667 0.06666667 0.06666667 0.06666667 0.06666667 0.06666667\nvtilde(1/vi)\n#> [1] 0.06666667\n\n# heterogeneous vi\nn <- 10 + rpois(k, 30 - 10)\nvi <- sim_vi(k = k, n1 = n)\nvtilde(1/vi)\n#> [1] 0.06266246\n```\n:::\n\n\n## What about $\\tilde{v}$?\n\nUsing simulations we can see that $\\tilde{v}$ with heterogenenous variances (i.e., sample sizes in this case) can be approximated by the central tendency of the sample size distribution. Note that we are fixing $\\sigma^2 = 1$ thus we are not including uncertainty.\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](meta-analysis-models_files/figure-revealjs/unnamed-chunk-34-1.svg){fig-align='center' width=960}\n:::\n:::\n\n\n## $H^2$\n\nThe $H^2$ is an alternative index of heterogeneity. Is calculated as:\n\n$$\nH^2 = \\frac{Q}{k - 1}\n$$\n\nWe defined $Q$ as the weighted sum of squares representing the total variability. $k - 1$ is the expected value of the $\\chi^2$ statistics (i.e., sum of squares) when $\\tau^2 = 0$ (or $\\lambda = 0$). \n\nThus $H^2$ is the ratio between total heterogeneity and sampling variability. Higher $H^2$ is associated with higher heterogeneity **relative** to the sampling variability. $H^2$ is not a measure of absolute heterogeneity.\n\n## $H^2$\n\nWhen we are fitting a RE model, the $I^2$ and $H^2$ equations are slightly different [@Higgins2002-fh]^[see also the `metafor` source code https://github.com/cran/metafor/blob/994d26a65455fac90760ad6a004ec1eaca5856b1/R/rma.uni.r#L2459C30-L2459C30].\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nk <- 100\nmu <- 0.5\ntau2 <- 0.1\nn <- 30\n\ndat <- sim_studies(k = k, es = mu, tau2 = tau2, n1 = n)\nfit_re <- rma(yi, vi, data = dat, method = \"REML\")\nfit_ee <- rma(yi, vi, data = dat, method = \"EE\")\n\n# H2 with EE model\n\ntheta_ee <- fit_ee$b[[1]] # weighted.mean(dat$yi, 1/dat$vi)\nwi <- 1/dat$vi\nQ <- with(dat, sum((1/vi)*(yi - theta_ee)^2))\nc(Q, fit_ee$QE) # same\n#> [1] 224.5983 224.5983\n\nc(H2 = fit_ee$QE / (fit_ee$k - fit_ee$p), H2_model = fit_ee$H2) # same\n#>       H2 H2_model \n#> 2.268669 2.268669\n\n# H2 with RE model\n\nvt <- vtilde(1/dat$vi)\nc(H2 = fit_re$tau2 / vt + 1, H2_model = fit_re$H2) # same\n#>       H2 H2_model \n#> 2.261768 2.261768\n```\n:::\n\n\n## Confidence Intervals\n\nWhat is reported in the model summary as `ci.lb` and `ci.ub` refers to the 95% confidence interval representing the uncertainty in estimating the effect (or a meta-regression parameter).\n\nWithout looking at the equations, let's try to implement this idea using simulations.\n\n- choose $k$, $\\tau^2$ and $n$\n- simulate data (several times) accordingly and fit the RE model\n- extract the estimated effect size\n- compare the simulated sampling distribution with the analytical result\n\n## Confidence Intervals\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nk <- 30\nn <- 30\ntau2 <- 0.05\nmu <- 0.5\nnsim <- 5e3\n\n# true parameters (see Borenstein, 2009; Chapter 29)\nvt <- 1/n + 1/n\nvs <- (vt + tau2)/ k\nse <- sqrt(vs)\n\nmui <- rep(NA, nsim)\n\nfor(i in 1:nsim){\n  dat <- sim_studies(k = k, es = mu, tau2 = tau2, n1 = n)\n  fit <- rma(yi, vi, data = dat)\n  mui[i] <- coef(fit)[1]\n}\n\n# standard error\nc(simulated = sd(mui), analytical = fit$se)\n#>  simulated analytical \n#> 0.06363308 0.06417368\n\n# confidence interval\nrbind(\n  \"simulated\"  = quantile(mui, c(0.05, 0.975)),\n  \"analytical\" = c(\"2.5%\" = fit$ci.lb, \"97.5%\" = fit$ci.ub)\n)\n#>                   5%     97.5%\n#> simulated  0.3938230 0.6236432\n#> analytical 0.4245134 0.6760696\n```\n:::\n\n\n## Confidence Intervals\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](meta-analysis-models_files/figure-revealjs/unnamed-chunk-37-1.svg){fig-align='center' width=960}\n:::\n:::\n\n\n## Confidence Intervals\n\nNow the equation for the 95% confidence interval should be more clear. The standard error is a function of the within study sampling variances (depending mainly on $n$), $\\tau^2$ and $k$. As we increase $k$ the standard error tends towards zero. \n\n$$\nCI = \\hat \\mu_{\\theta} \\pm z SE_{\\mu_{\\theta}}\n$$\n\n$$\nSE_{\\mu_{\\theta}} = \\sqrt{\\frac{1}{\\sum^{k}_{i = 1}w^{\\star}_i}}\n$$\n\n$$\nw^{\\star}_i = \\frac{1}{\\sigma^2_i + \\tau^2}\n$$\n\n## Confidence Intervals\n\nWe can also see it analytically, there is a huge impact of $k$.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](meta-analysis-models_files/figure-revealjs/unnamed-chunk-38-1.svg){fig-align='center' width=960}\n:::\n:::\n\n\n## Prediction intervals (PI)\n\nWe could say that the CI is not completely taking into account the between-study heterogeneity ($\\tau^2$). After a meta-analysis we would like to know how confident we are in the parameters estimation BUT also **what would be the expected effect running a new experiment tomorrow?**.\n\nThe **prediction interval** [@IntHout2016-sz; @Riley2011-hp] is exactly the range of effects that I expect in predicting a new study.\n\n## PI for a sample mean\n\nTo understand the concept, let's assume to have a sample $X$ of size $n$ and we estimate the mean $\\overline X$. The PI is calculated as^[Notice that the equation, in particular the usage of $t$ vs $z$ depends on assuming $s_x$ to be known or estimated. See [https://online.stat.psu.edu/stat501/lesson/3/3.3](https://online.stat.psu.edu/stat501/lesson/3/3.3), [https://en.wikipedia.org/wiki/Prediction_interval](https://en.wikipedia.org/wiki/Prediction_interval) and [https://www.bryanshalloway.com/2021/03/18/intuition-on-uncertainty-of-predictions-introduction-to-prediction-intervals/](https://www.bryanshalloway.com/2021/03/18/intuition-on-uncertainty-of-predictions-introduction-to-prediction-intervals/)]:\n\n$$\nPI = \\overline X \\pm t_{\\alpha/2} s_x \\sqrt{1 + \\frac{1}{n}}\n$$\n\nWhere $s$ is the sample standard deviation. Basically we are combining the uncertainty in estimating $\\overline X$ (i.e, $\\frac{s_x}{n}$) with the standard deviation of the data $s_x$. Compare it with the confidence interval containing only $\\frac{s_x}{n}$.\n\n## PI in meta-analysis\n\nFor meta-analysis the equation^[When a $t$ distribution is assumed, the quantiles are calculated using $k - 2$ degrees of freedom] is conceptually similar but with different quantities.\n\n$$\nPI = \\hat \\mu_{\\theta} \\pm z \\sqrt{\\tau^2 + SE_{\\mu_{\\theta}}}\n$$\n\nBasically we are combining all the sources of uncertainty. As long as $\\tau^2 \\neq 0$ the PI is greater than the CI (in the EE model they are the same). Thus even with very precise $\\mu_{\\theta}$ estimation, large $\\tau^2$ leads to uncertain predictions.\n\n## PI in meta-analysis\n\nIn R the PI can be calculated using `predict()`. By default the model assume a standard normal distribution thus using $z$ scores. To use the @Riley2011-hp approach ($t$ distribution) the model need to be fitted using `test = \"t\"`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nk <- 100\ndat <- sim_studies(k = k, es = 0.5, tau2 = 0.1, n1 = 30)\nfit_z <- rma(yi, vi, data = dat, test = \"z\") # test = \"z\" is the default\npredict(fit_z) # notice pi.ub/pi.lb vs ci.ub/ci.lb\n#> \n#>    pred     se  ci.lb  ci.ub   pi.lb  pi.ub \n#>  0.4766 0.0410 0.3963 0.5570 -0.1581 1.1114\n# manually\nfit_z$b[[1]] + qnorm(c(0.025, 0.975)) * sqrt(fit_z$se^2 + fit_z$tau2)\n#> [1] -0.1581267  1.1113777\n\nfit_t <- rma(yi, vi, data = dat, test = \"t\")\npredict(fit_t) # notice pi.ub/pi.lb vs ci.ub/ci.lb\n#> \n#>    pred     se  ci.lb  ci.ub   pi.lb  pi.ub \n#>  0.4766 0.0410 0.3953 0.5580 -0.1660 1.1192\n# manually\nfit_z$b[[1]] + qt(c(0.025, 0.975), k - 2) * sqrt(fit_t$se^2 + fit_t$tau2)\n#> [1] -0.1660623  1.1193133\n```\n:::\n\n\n## References <button class=\"btn\"><i class=\"fa fa-download\"></i><a href=\"data:text/x-bibtex;base64,@ARTICLE{Viechtbauer2005-zt,
  title = {Bias and efficiency of meta-analytic variance estimators in the
  random-effects model},
  author = {Viechtbauer, Wolfgang},
  journaltitle = {Journal of educational and behavioral statistics: a quarterly
  publication sponsored by the American Educational Research Association and the
  American Statistical Association},
  publisher = {American Educational Research Association (AERA)},
  volume = {30},
  issue = {3},
  pages = {261-293},
  date = {2005-09},
  doi = {10.3102/10769986030003261},
  issn = {1076-9986,1935-1054},
  abstract = {The meta-analytic random effects model assumes that the
  variability in effect size estimates drawn from a set of studies can be
  decomposed into two parts: heterogeneity due to random population effects and
  sampling variance. In this context, the usual goal is to estimate the central
  tendency and the amount of heterogeneity in the population effect sizes. The
  amount of heterogeneity in a set of effect sizes has implications regarding
  the interpretation of the meta-analytic findings and often serves as an
  indicator for the presence of potential moderator variables. Five population
  heterogeneity estimators were compared in this article analytically and via
  Monte Carlo simulations with respect to their bias and efficiency.},
  url = {https://scholar.google.com/citations?view_op=view_citation&hl=en&user=J89RXJkAAAAJ&citation_for_view=J89RXJkAAAAJ:qjMakFHDy7sC},
  language = {en}
}

@ARTICLE{Veroniki2016-zs,
  title = {Methods to estimate the between-study variance and its uncertainty in
  meta-analysis},
  author = {Veroniki, Areti Angeliki and Jackson, Dan and Viechtbauer, Wolfgang
  and Bender, Ralf and Bowden, Jack and Knapp, Guido and Kuss, Oliver and
  Higgins, Julian P T and Langan, Dean and Salanti, Georgia},
  journaltitle = {Research synthesis methods},
  publisher = {Wiley},
  volume = {7},
  issue = {1},
  pages = {55-79},
  date = {2016-03},
  doi = {10.1002/jrsm.1164},
  pmc = {PMC4950030},
  pmid = {26332144},
  issn = {1759-2879,1759-2887},
  abstract = {Meta-analyses are typically used to estimate the overall/mean of
  an outcome of interest. However, inference about between-study variability,
  which is typically modelled using a between-study variance parameter, is
  usually an additional aim. The DerSimonian and Laird method, currently widely
  used by default to estimate the between-study variance, has been long
  challenged. Our aim is to identify known methods for estimation of the
  between-study variance and its corresponding uncertainty, and to summarise the
  simulation and empirical evidence that compares them. We identified 16
  estimators for the between-study variance, seven methods to calculate
  confidence intervals, and several comparative studies. Simulation studies
  suggest that for both dichotomous and continuous data the estimator proposed
  by Paule and Mandel and for continuous data the restricted maximum likelihood
  estimator are better alternatives to estimate the between-study variance.
  Based on the scenarios and results presented in the published studies, we
  recommend the Q-profile method and the alternative approach based on a
  'generalised Cochran between-study variance statistic' to compute
  corresponding confidence intervals around the resulting estimates. Our
  recommendations are based on a qualitative evaluation of the existing
  literature and expert consensus. Evidence-based recommendations require an
  extensive simulation study where all methods would be compared under the same
  scenarios.},
  url = {https://scholar.google.com/citations?view_op=view_citation&hl=en&user=J89RXJkAAAAJ&citation_for_view=J89RXJkAAAAJ:Xc-mKOjpdrwC},
  keywords = {bias; confidence interval; coverage probability; heterogeneity;
  mean squared error},
  language = {en}
}

@BOOK{Harrer2021-go,
  title = {Doing meta-analysis with R: A hands-on guide},
  author = {Harrer, Mathias and Cuijpers, Pim and Furukawa, Toshi and Ebert,
  David},
  publisher = {CRC Press},
  location = {London, England},
  edition = {1st},
  date = {2021-09-13},
  pagetotal = {474},
  isbn = {9780367610074},
  language = {en}
}

@ARTICLE{Brannick2019-hv,
  title = {Bias and precision of alternate estimators in meta-analysis: Benefits
  of blending Schmidt-Hunter and Hedges approaches},
  author = {Brannick, Michael T and Potter, Sean M and Benitez, Bryan and
  Morris, Scott B},
  journaltitle = {Organizational research methods},
  publisher = {SAGE Publications},
  volume = {22},
  issue = {2},
  pages = {490-514},
  date = {2019-04},
  doi = {10.1177/1094428117741966},
  issn = {1094-4281,1552-7425},
  abstract = {We describe a new estimator (labeled Morris) for meta-analysis.
  The Morris estimator combines elements of both the Schmidt-Hunter and Hedges
  estimators. The new estimator is compared to (a) the Schmidt-Hunter estimator,
  (b) the Schmidt-Hunter estimator with variance correction for the number of
  studies (“ k correction”), (c) the Hedges random-effects estimator, and (d)
  the Bonett unit weights estimator in a Monte Carlo simulation. The simulation
  was designed to represent realistic conditions faced by researchers, including
  population random-effects distributions, numbers of studies, and skewed sample
  size distributions. The simulation was used to evaluate the estimators with
  respect to bias, coverage of the 95\% confidence interval of the mean, and
  root mean square error of estimates of the population mean. We also evaluated
  the quality of credibility intervals. Overall, the new estimator provides
  better coverage and slightly better credibility values than other commonly
  used methods. Thus it has advantages of both commonly used approaches without
  the apparent disadvantages. The new estimator can be implemented easily with
  existing software; software used in the study is available online, and an
  example is included in the appendix in the Supplemental Material available
  online.},
  url = {https://journals.sagepub.com/doi/abs/10.1177/1094428117741966},
  language = {en}
}

@MISC{Borenstein2009-mo,
  title = {Introduction to {Meta-Analysis}},
  author = {Borenstein, Michael and Hedges, Larry V and Higgins, Julian P T and
  Rothstein, Hannah R},
  date = {2009},
  doi = {10.1002/9780470743386},
  url = {http://dx.doi.org/10.1002/9780470743386},
  keywords = {MA Unconscious WM}
}

@ARTICLE{Riley2011-hp,
  title = {Interpretation of random effects meta-analyses},
  author = {Riley, Richard D and Higgins, Julian P T and Deeks, Jonathan J},
  journaltitle = {BMJ},
  volume = {342},
  pages = {d549},
  date = {2011-02-10},
  doi = {10.1136/bmj.d549},
  pmid = {21310794},
  issn = {0959-8138,1756-1833},
  url = {http://dx.doi.org/10.1136/bmj.d549},
  keywords = {MA Unconscious WM},
  language = {en}
}

@ARTICLE{IntHout2016-sz,
  title = {Plea for routinely presenting prediction intervals in meta-analysis},
  author = {IntHout, Joanna and Ioannidis, John P A and Rovers, Maroeska M and
  Goeman, Jelle J},
  journaltitle = {BMJ open},
  volume = {6},
  issue = {7},
  pages = {e010247},
  date = {2016-07-12},
  doi = {10.1136/bmjopen-2015-010247},
  pmc = {PMC4947751},
  pmid = {27406637},
  issn = {2044-6055},
  abstract = {OBJECTIVES: Evaluating the variation in the strength of the effect
  across studies is a key feature of meta-analyses. This variability is
  reflected by measures like τ(2) or I(2), but their clinical interpretation is
  not straightforward. A prediction interval is less complicated: it presents
  the expected range of true effects in similar studies. We aimed to show the
  advantages of having the prediction interval routinely reported in
  meta-analyses. DESIGN: We show how the prediction interval can help understand
  the uncertainty about whether an intervention works or not. To evaluate the
  implications of using this interval to interpret the results, we selected the
  first meta-analysis per intervention review of the Cochrane Database of
  Systematic Reviews Issues 2009-2013 with a dichotomous (n=2009) or continuous
  (n=1254) outcome, and generated 95\% prediction intervals for them. RESULTS:
  In 72.4\% of 479 statistically significant (random-effects p0), the 95\%
  prediction interval suggested that the intervention effect could be null or
  even be in the opposite direction. In 20.3\% of those 479 meta-analyses, the
  prediction interval showed that the effect could be completely opposite to the
  point estimate of the meta-analysis. We demonstrate also how the prediction
  interval can be used to calculate the probability that a new trial will show a
  negative effect and to improve the calculations of the power of a new trial.
  CONCLUSIONS: The prediction interval reflects the variation in treatment
  effects over different settings, including what effect is to be expected in
  future patients, such as the patients that a clinician is interested to treat.
  Prediction intervals should be routinely reported to allow more informative
  inferences in meta-analyses.},
  url = {http://dx.doi.org/10.1136/bmjopen-2015-010247},
  keywords = {Clinical trial; Cochrane Database of Systematic Reviews;
  Heterogeneity; Meta-analysis; Prediction interval; Random effects;MA
  Unconscious WM;Bayesian Statistics},
  language = {en}
}

@ARTICLE{Higgins2002-fh,
  title = {Quantifying heterogeneity in a meta-analysis},
  author = {Higgins, Julian P T and Thompson, Simon G},
  journaltitle = {Statistics in medicine},
  volume = {21},
  issue = {11},
  pages = {1539-1558},
  date = {2002-06-15},
  doi = {10.1002/sim.1186},
  pmid = {12111919},
  issn = {0277-6715},
  abstract = {The extent of heterogeneity in a meta-analysis partly determines
  the difficulty in drawing overall conclusions. This extent may be measured by
  estimating a between-study variance, but interpretation is then specific to a
  particular treatment effect metric. A test for the existence of heterogeneity
  exists, but depends on the number of studies in the meta-analysis. We develop
  measures of the impact of heterogeneity on a meta-analysis, from mathematical
  criteria, that are independent of the number of studies and the treatment
  effect metric. We derive and propose three suitable statistics: H is the
  square root of the chi2 heterogeneity statistic divided by its degrees of
  freedom; R is the ratio of the standard error of the underlying mean from a
  random effects meta-analysis to the standard error of a fixed effect
  meta-analytic estimate, and I2 is a transformation of (H) that describes the
  proportion of total variation in study estimates that is due to heterogeneity.
  We discuss interpretation, interval estimates and other properties of these
  measures and examine them in five example data sets showing different amounts
  of heterogeneity. We conclude that H and I2, which can usually be calculated
  for published meta-analyses, are particularly useful summaries of the impact
  of heterogeneity. One or both should be presented in published meta-analyses
  in preference to the test for heterogeneity.},
  url = {http://dx.doi.org/10.1002/sim.1186},
  language = {en}
}

@ARTICLE{Hedges2019-ry,
  title = {Statistical analyses for studying replication: Meta-analytic
  perspectives},
  author = {Hedges, Larry V and Schauer, Jacob M},
  journaltitle = {Psychological methods},
  publisher = {American Psychological Association (APA)},
  volume = {24},
  issue = {5},
  pages = {557-570},
  date = {2019-10},
  doi = {10.1037/met0000189},
  pmid = {30070547},
  issn = {1082-989X,1939-1463},
  abstract = {Formal empirical assessments of replication have recently become
  more prominent in several areas of science, including psychology. These
  assessments have used different statistical approaches to determine if a
  finding has been replicated. The purpose of this article is to provide several
  alternative conceptual frameworks that lead to different statistical analyses
  to test hypotheses about replication. All of these analyses are based on
  statistical methods used in meta-analysis. The differences among the methods
  described involve whether the burden of proof is placed on replication or
  nonreplication, whether replication is exact or allows for a small amount of
  "negligible heterogeneity," and whether the studies observed are assumed to be
  fixed (constituting the entire body of relevant evidence) or are a sample from
  a universe of possibly relevant studies. The statistical power of each of
  these tests is computed and shown to be low in many cases, raising issues of
  the interpretability of tests for replication. (PsycINFO Database Record (c)
  2019 APA, all rights reserved).},
  url = {http://dx.doi.org/10.1037/met0000189},
  file = {Hedges and Schauer 2019 - Statistical analyses for studying replication - Meta-analytic perspectives.pdf},
  keywords = {replication-methods},
  language = {en}
}

\" download=\"refs_to_download.bib\"> Download .bib file</a></button> {.refs}\n\n::: {#refs}\n:::",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}